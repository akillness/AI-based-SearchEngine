"""
Ollama DeepSeek ê¸°ë°˜ AI ê²€ìƒ‰ì—”ì§„ í”„ë ˆì„ì›Œí¬
===========================================

ì´ ëª¨ë“ˆì€ Ollama DeepSeek ëª¨ë¸ê³¼ Brave Search APIë¥¼ í™œìš©í•˜ì—¬
ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” ì¸í…”ë¦¬ì „íŠ¸í•œ ì›¹ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.
BERTì™€ FAISSë¥¼ í™œìš©í•œ ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰ë„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import os
import json
import requests
import asyncio
import httpx
from newspaper import Article
import numpy as np
from transformers import BertTokenizer, BertModel
import faiss
import torch
import logging
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
import time
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    id: int
    title: str
    url: str
    content: str
    description: str = ""
    score: float = 0.0

@dataclass
class VectorSearchResult:
    """ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    document: str
    distance: float
    similarity_score: float = 0.0

class OllamaClient:
    """Ollama ì„œë²„ì™€ í†µì‹ í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, base_url: str = "http://localhost:11434", model: str = "deepseek-r1:14b"):
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.session = requests.Session()
        
    def is_available(self) -> bool:
        """Ollama ì„œë²„ ê°€ìš©ì„± í™•ì¸"""
        try:
            response = self.session.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def list_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            response = self.session.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                data = response.json()
                return [model['name'] for model in data.get('models', [])]
        except Exception as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []
    
    def pull_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_name}")
            response = self.session.post(
                f"{self.base_url}/api/pull",
                json={"name": model_name},
                stream=True,
                timeout=300
            )
            
            for line in response.iter_lines():
                if line:
                    data = json.loads(line.decode('utf-8'))
                    if data.get('status'):
                        logger.info(f"ë‹¤ìš´ë¡œë“œ ìƒíƒœ: {data['status']}")
                    if data.get('error'):
                        logger.error(f"ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {data['error']}")
                        return False
            
            logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_name}")
            return True
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.3) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": temperature,
                    "top_k": 40,
                    "top_p": 0.9
                }
            }
            
            response = self.session.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                return data.get('response', '').strip()
            else:
                logger.error(f"ìƒì„± ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                return ""
                
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""
    
    async def generate_async(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.3) -> str:
        """ë¹„ë™ê¸° í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": temperature,
                    "top_k": 40,
                    "top_p": 0.9
                }
            }
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json=payload
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data.get('response', '').strip()
                else:
                    logger.error(f"ë¹„ë™ê¸° ìƒì„± ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                    return ""
                    
        except Exception as e:
            logger.error(f"ë¹„ë™ê¸° í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

class VectorSearchModule:
    """BERTì™€ FAISSë¥¼ í™œìš©í•œ ë²¡í„° ê²€ìƒ‰ ëª¨ë“ˆ"""
    
    def __init__(self, model_name='bert-base-uncased'):
        """ë²¡í„° ê²€ìƒ‰ ëª¨ë“ˆ ì´ˆê¸°í™”"""
        try:
            self.tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir="./cache")
            self.model = BertModel.from_pretrained(model_name, cache_dir="./cache")
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.model.to(self.device)
            logger.info(f"ë²¡í„° ê²€ìƒ‰ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ: {model_name} on {self.device}")
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.tokenizer = None
            self.model = None

    def text_to_vector(self, text: str) -> Optional[np.ndarray]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if not self.tokenizer or not self.model:
            return None
        
        try:
            inputs = self.tokenizer(text, return_tensors="pt", max_length=512, 
                                  truncation=True, padding=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            vector = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
            return vector
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            return None

    def build_index(self, corpus: List[str]) -> Tuple[Optional[faiss.Index], Optional[List[str]]]:
        """í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ì— ëŒ€í•´ FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        if not corpus:
            return None, None
        
        vectors = []
        for i, doc in enumerate(corpus):
            if i % 10 == 0:
                logger.info(f"ë²¡í„°í™” ì§„í–‰: {i+1}/{len(corpus)}")
            vec = self.text_to_vector(doc)
            if vec is not None:
                vectors.append(vec)
        
        if not vectors:
            return None, None

        vectors_np = np.vstack(vectors)
        dimension = vectors_np.shape[1]
        
        index = faiss.IndexFlatL2(dimension)
        index.add(vectors_np)
        
        logger.info(f"FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„°")
        return index, corpus

    def search(self, query: str, index: faiss.Index, corpus: List[str], k: int = 3) -> List[VectorSearchResult]:
        """ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰"""
        if not index or not corpus:
            return []
        
        query_vector = self.text_to_vector(query)
        if query_vector is None:
            return []
        
        distances, indices = index.search(query_vector, k)
        
        results = []
        for i in range(len(indices[0])):
            idx = indices[0][i]
            if 0 <= idx < len(corpus):
                distance = float(distances[0][i])
                similarity = 1.0 / (1.0 + distance)
                
                results.append(VectorSearchResult(
                    document=corpus[idx],
                    distance=distance,
                    similarity_score=similarity
                ))
        
        return results

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        vec1 = self.text_to_vector(text1)
        vec2 = self.text_to_vector(text2)
        
        if vec1 is None or vec2 is None:
            return 0.0
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        dot_product = np.dot(vec1.flatten(), vec2.flatten())
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(dot_product / (norm1 * norm2))

class OllamaSearchEngine:
    """Ollama DeepSeek ê¸°ë°˜ AI ê²€ìƒ‰ì—”ì§„ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, ollama_url: str = "http://localhost:11434", 
                 model: str = "deepseek-r1:14b", 
                 brave_api_key: str = None):
        """ê²€ìƒ‰ì—”ì§„ ì´ˆê¸°í™”"""
        self.ollama_client = OllamaClient(ollama_url, model)
        self.brave_api_key = brave_api_key or os.environ.get("BRAVE_API_KEY")
        self.vector_module = VectorSearchModule()
        
        # Ollama ì„œë²„ í™•ì¸ ë° ëª¨ë¸ ì¤€ë¹„
        self._check_ollama_setup()

    def _check_ollama_setup(self):
        """Ollama ì„œë²„ ë° ëª¨ë¸ ì„¤ì • í™•ì¸"""
        if not self.ollama_client.is_available():
            logger.error("Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. http://localhost:11434ì—ì„œ Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return
        
        available_models = self.ollama_client.list_models()
        if self.ollama_client.model not in available_models:
            logger.warning(f"ëª¨ë¸ {self.ollama_client.model}ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
            if not self.ollama_client.pull_model(self.ollama_client.model):
                logger.error("ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                # ëŒ€ì²´ ëª¨ë¸ ì‹œë„
                for alt_model in ["deepseek-coder:1.3b", "llama2:7b", "codellama:7b"]:
                    if alt_model in available_models:
                        logger.info(f"ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©: {alt_model}")
                        self.ollama_client.model = alt_model
                        break
        else:
            logger.info(f"ëª¨ë¸ {self.ollama_client.model} ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ")

    def generate_related_queries(self, query: str) -> List[str]:
        """DeepSeekì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ì¿¼ë¦¬ ìƒì„±"""
        prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê²€ìƒ‰ ì¿¼ë¦¬ 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”: "{query}"

ê° ì¿¼ë¦¬ëŠ” ì›ë˜ ì§ˆë¬¸ì˜ ë‹¤ë¥¸ ì¸¡ë©´ì´ë‚˜ ê´€ë ¨ ê°œë…ì„ ë‹¤ë¤„ì•¼ í•©ë‹ˆë‹¤.
JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ í˜•ì‹:
["ê´€ë ¨ ì§ˆë¬¸ 1", "ê´€ë ¨ ì§ˆë¬¸ 2", "ê´€ë ¨ ì§ˆë¬¸ 3", "ê´€ë ¨ ì§ˆë¬¸ 4", "ê´€ë ¨ ì§ˆë¬¸ 5"]

ì‘ë‹µ:"""

        try:
            response = self.ollama_client.generate(prompt, max_tokens=300, temperature=0.7)
            
            # JSON íŒŒì‹± ì‹œë„
            start_idx = response.find('[')
            end_idx = response.rfind(']') + 1
            
            if start_idx != -1 and end_idx != 0:
                json_str = response[start_idx:end_idx]
                related_queries = json.loads(json_str)
                
                if isinstance(related_queries, list):
                    return [query] + related_queries[:4]  # ì›ë³¸ í¬í•¨ ìµœëŒ€ 5ê°œ
            
            logger.warning("ê´€ë ¨ ì¿¼ë¦¬ JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ë§Œ ì‚¬ìš©")
            return [query]
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return [query]

    def get_search_results(self, query: str) -> List[Dict]:
        """Brave Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        if not self.brave_api_key:
            logger.warning("Brave API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        url = "https://api.search.brave.com/res/v1/web/search"
        params = {"q": query, "count": 5}
        headers = {"X-Subscription-Token": self.brave_api_key}
        
        try:
            response = requests.get(url, params=params, headers=headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            return data.get("web", {}).get("results", [])
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []

    def get_url_content(self, url: str) -> str:
        """URLì˜ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            article = Article(url)
            article.download()
            article.parse()
            return article.text
        except Exception as e:
            logger.error(f"URL ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
            return ""

    def generate_response(self, query: str, search_results: List[SearchResult]) -> str:
        """DeepSeekì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
        if not search_results:
            return "ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for result in search_results:
            content_snippet = result.content[:500] + "..." if len(result.content) > 500 else result.content
            context_parts.append(f"[{result.id}] {result.title}: {content_snippet}")
        
        context = "\n".join(context_parts)
        
        prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ê²€ìƒ‰ëœ ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ì¡°ê±´:
1. ì œê³µëœ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ê±°ë‚˜ í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ ê·¸ë ‡ê²Œ ëª…ì‹œí•˜ì„¸ìš”  
3. ë‹µë³€ì— ì‚¬ìš©í•œ ì •ë³´ì˜ ì¶œì²˜ë¥¼ [ë²ˆí˜¸] í˜•ì‹ìœ¼ë¡œ ì¸ìš©í•˜ì„¸ìš”
4. ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”

ë‹µë³€:"""

        try:
            response = self.ollama_client.generate(prompt, max_tokens=1000, temperature=0.3)
            return response if response else "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def search_and_answer(self, query: str) -> Dict[str, Any]:
        """ì›¹ ê²€ìƒ‰ ë° DeepSeek ë‹µë³€ ìƒì„± íŒŒì´í”„ë¼ì¸"""
        logger.info(f"ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
        start_time = time.time()
        
        # 1. ê´€ë ¨ ì¿¼ë¦¬ ìƒì„±
        logger.info("ê´€ë ¨ ì¿¼ë¦¬ ìƒì„± ì¤‘...")
        related_queries = self.generate_related_queries(query)
        logger.info(f"ìƒì„±ëœ ê´€ë ¨ ì¿¼ë¦¬: {related_queries}")
        
        # 2. ê²€ìƒ‰ ì‹¤í–‰
        all_results = []
        unique_urls = set()
        
        for search_query in related_queries[:3]:  # ìµœëŒ€ 3ê°œ ì¿¼ë¦¬
            logger.info(f"ê²€ìƒ‰ ì¤‘: {search_query}")
            results = self.get_search_results(search_query)
            
            for result in results:
                url = result.get("url")
                if url and url not in unique_urls:
                    unique_urls.add(url)
                    logger.info(f"ì½˜í…ì¸  ì¶”ì¶œ ì¤‘: {url}")
                    content = self.get_url_content(url)
                    
                    if content:
                        all_results.append(SearchResult(
                            id=len(all_results) + 1,
                            title=result.get("title", ""),
                            url=url,
                            content=content,
                            description=result.get("description", "")
                        ))
                
                if len(all_results) >= 5:  # ìµœëŒ€ 5ê°œ ê²°ê³¼
                    break
            
            if len(all_results) >= 5:
                break
        
        logger.info(f"ìˆ˜ì§‘ëœ ê²°ê³¼: {len(all_results)}ê°œ")
        
        # 3. ë‹µë³€ ìƒì„±
        logger.info("ë‹µë³€ ìƒì„± ì¤‘...")
        answer = self.generate_response(query, all_results)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        result = {
            "query": query,
            "answer": answer,
            "sources": [
                {
                    "id": r.id,
                    "title": r.title,
                    "url": r.url,
                    "description": r.description
                } for r in all_results
            ],
            "related_queries": related_queries[1:],  # ì›ë³¸ ì¿¼ë¦¬ ì œì™¸
            "processing_time": round(processing_time, 2),
            "timestamp": datetime.now().isoformat(),
            "model_used": self.ollama_client.model
        }
        
        return result

def setup_ollama():
    """Ollama ì„¤ì¹˜ ë° ì„¤ì • ê°€ì´ë“œ"""
    print("""
ğŸ”§ Ollama ì„¤ì¹˜ ê°€ì´ë“œ
====================

1. Ollama ì„¤ì¹˜:
   - macOS: brew install ollama
   - Linux: curl -fsSL https://ollama.com/install.sh | sh
   - Windows: https://ollama.com/download ì—ì„œ ë‹¤ìš´ë¡œë“œ

2. Ollama ì„œë²„ ì‹œì‘:
   ollama serve

3. DeepSeek ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:
   ollama pull deepseek-coder:6.7b
   
   ë˜ëŠ” ë” ì‘ì€ ëª¨ë¸:
   ollama pull deepseek-coder:1.3b

4. ëª¨ë¸ í™•ì¸:
   ollama list

ì„¤ì¹˜ í›„ ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.
""")

def main():
    """ë©”ì¸ í•¨ìˆ˜ - ê²€ìƒ‰ì—”ì§„ ë°ëª¨"""
    print("ğŸ¤– Ollama DeepSeek ê¸°ë°˜ AI ê²€ìƒ‰ì—”ì§„")
    print("=" * 50)
    
    # Ollama í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸
    ollama_client = OllamaClient()
    
    if not ollama_client.is_available():
        print("âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        setup_ollama()
        return
    
    print("âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
    models = ollama_client.list_models()
    print(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {models}")
    
    # ê²€ìƒ‰ì—”ì§„ ì´ˆê¸°í™”
    engine = OllamaSearchEngine()
    
    # ì›¹ ê²€ìƒ‰ ë°ëª¨
    print("\n--- ì›¹ ê²€ìƒ‰ + DeepSeek ë‹µë³€ ë°ëª¨ ---")
    if engine.brave_api_key:
        query = input("ê²€ìƒ‰í•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if query.strip():
            print("\nğŸ” ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘...")
            try:
                result = engine.search_and_answer(query)
                
                print(f"\nğŸ“ ë‹µë³€:")
                print("=" * 40)
                print(result['answer'])
                
                print(f"\nğŸ“š ì¶œì²˜ ({len(result['sources'])}ê°œ):")
                print("-" * 40)
                for source in result['sources'][:3]:
                    print(f"[{source['id']}] {source['title']}")
                    print(f"    {source['url']}")
                
                print(f"\nğŸ”— ê´€ë ¨ ì¿¼ë¦¬:")
                print("-" * 40)
                for rq in result['related_queries']:
                    print(f"  â€¢ {rq}")
                
                print(f"\nâ±ï¸  ì²˜ë¦¬ ì‹œê°„: {result['processing_time']}ì´ˆ")
                print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {result['model_used']}")
                
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        print("âš ï¸  Brave API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì›¹ ê²€ìƒ‰ ë°ëª¨ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        print("í™˜ê²½ ë³€ìˆ˜ BRAVE_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ .env íŒŒì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    # ë²¡í„° ê²€ìƒ‰ ë°ëª¨
    print("\n--- ë²¡í„° ê²€ìƒ‰ ë°ëª¨ ---")
    if engine.vector_module.model:
        sample_corpus = [
            "ê¸°í›„ ë³€í™”ëŠ” ì§€êµ¬ ì˜¨ë‚œí™”ë¡œ ì¸í•œ ì „ ì§€êµ¬ì  í™˜ê²½ ë³€í™”ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.",
            "ì¸ê³µì§€ëŠ¥ì€ ê¸°ê³„ê°€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ì—¬ í•™ìŠµí•˜ê³  ì¶”ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
            "Pythonì€ ë°ì´í„° ê³¼í•™ê³¼ ì›¹ ê°œë°œì— ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
            "ì—í íƒ‘ì€ í”„ë‘ìŠ¤ íŒŒë¦¬ì— ìœ„ì¹˜í•œ ì² ì œ ê²©ì êµ¬ì¡°ì˜ íƒ‘ì…ë‹ˆë‹¤.",
            "ê´‘í•©ì„±ì€ ì‹ë¬¼ì´ ë¹› ì—ë„ˆì§€ë¥¼ í™”í•™ ì—ë„ˆì§€ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤."
        ]
        
        print(f"ğŸ“Š ìƒ˜í”Œ ì½”í¼ìŠ¤ ({len(sample_corpus)}ê°œ ë¬¸ì„œ)ë¡œ ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        index, corpus = engine.vector_module.build_index(sample_corpus)
        
        if index:
            vector_query = input("ë²¡í„° ê²€ìƒ‰í•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
            if vector_query.strip():
                results = engine.vector_module.search(vector_query, index, corpus, k=2)
                print("\nğŸ¯ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼:")
                print("-" * 40)
                for i, result in enumerate(results, 1):
                    print(f"{i}. ë¬¸ì„œ: {result.document}")
                    print(f"   ìœ ì‚¬ë„: {result.similarity_score:.4f}")
                    print(f"   ê±°ë¦¬: {result.distance:.4f}\n")
    
    # DeepSeek ì§ì ‘ í…ŒìŠ¤íŠ¸
    print("\n--- DeepSeek ëª¨ë¸ ì§ì ‘ í…ŒìŠ¤íŠ¸ ---")
    test_query = input("DeepSeekì—ê²Œ ì§ì ‘ ì§ˆë¬¸í•˜ì„¸ìš”: ")
    if test_query.strip():
        print("\nğŸ¤– DeepSeek ì‘ë‹µ:")
        print("-" * 40)
        response = engine.ollama_client.generate(
            f"ë‹¤ìŒ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ ë‹µë³€í•´ì£¼ì„¸ìš”: {test_query}",
            max_tokens=500
        )
        print(response)
    
    print("\nâœ¨ ê²€ìƒ‰ì—”ì§„ ë°ëª¨ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 